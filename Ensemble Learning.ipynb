{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6392,"databundleVersionId":44054,"sourceType":"competition"},{"sourceId":7036956,"sourceType":"datasetVersion","datasetId":4048446}],"dockerImageVersionId":30558,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom datetime import datetime, date\nfrom operator import le, eq\nimport gc\nfrom sklearn import model_selection, preprocessing\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LinearRegression\nimport zipfile\nimport pickle\nfrom sklearn.model_selection import ShuffleSplit, cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.ensemble import AdaBoostRegressor, RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.tree import DecisionTreeRegressor\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.preprocessing import StandardScaler\n\n\nRS = 20170501\nnp.random.seed(RS)\n\n# Extracting the zip files\n\nwith zipfile.ZipFile('../input/sberbank-russian-housing-market/train.csv.zip', 'r') as zip_ref:\n    zip_ref.extractall('./')\n    \nwith zipfile.ZipFile('../input/sberbank-russian-housing-market/test.csv.zip', 'r') as zip_ref:\n    zip_ref.extractall('./')\n\nwith zipfile.ZipFile('../input/sberbank-russian-housing-market/macro.csv.zip', 'r') as zip_ref:\n    zip_ref.extractall('./')\n\n# Data Cleaning\n\n#Data importing\ntrainDf = pd.read_csv('train.csv').set_index('id')\ntestDf = pd.read_csv('test.csv').set_index('id')\ntestDf['isTrain'] = 0\ntrainDf['isTrain'] = 1\nallDf = pd.concat([trainDf,testDf])\n\n# Change price by rate\nallDf['timestamp'] = pd.to_datetime(allDf['timestamp'])\n\nallDf['apartment_name'] = allDf.sub_area + allDf['metro_km_avto'].astype(str)\neco_map = {'excellent':4, 'good':3, 'satisfactory':2, 'poor':1, 'no data':0}\nallDf['ecology'] = allDf['ecology'].map(eco_map)\n#encode subarea in order\n# price_by_area = allDf['price_doc'].groupby(allDf.sub_area).mean().sort_values()\n# area_dict = {}\n# for i in range(0,price_by_area.shape[0]):\n#    area_dict[price_by_area.index[i]] = i\n# allDf['sub_area'] = allDf['sub_area'].map(area_dict)\nfor c in allDf.columns:\n    if allDf[c].dtype == 'object':\n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(allDf[c].values))\n        allDf[c] = lbl.transform(list(allDf[c].values))\n\ntrain_df=train_df[(train_df.price_doc>1e6) & (train_df.price_doc!=2e6) & (train_df.price_doc!=3e6)]\ntrain_df.loc[(train_df.product_type=='Investment') & (train_df.build_year<2000),'price_doc']*=0.895\ntrain_df.loc[train_df.product_type!='Investment','price_doc']*=0.96\n###Dealing with Outlier###\nallDf.loc[allDf.full_sq>2000,'full_sq'] = np.nan\nallDf.loc[allDf.full_sq<3,'full_sq'] = np.nan\nallDf.loc[allDf.life_sq>500,'life_sq'] = np.nan\nallDf.loc[allDf.life_sq<3,'life_sq'] = np.nan\n# allDf['lifesq_to_fullsq'] = 0 # 0 for normal, 1 for close,2 for outlier\nallDf.loc[allDf.life_sq>0.8*allDf.full_sq,'life_sq'] = np.nan\n# allDf.ix[allDf.life_sq>allDf.full_sq,['life_sq','lifesq_to_fullsq']] = np.nan, 2\nallDf.loc[allDf.kitch_sq>=allDf.life_sq,'kitch_sq'] = np.nan\nallDf.loc[allDf.kitch_sq>500,'kitch_sq'] = np.nan\nallDf.loc[allDf.kitch_sq<2,'kitch_sq'] = np.nan\nallDf.loc[allDf.state>30,'state'] = np.nan\nallDf.loc[allDf.build_year<1800,'build_year'] = np.nan\nallDf.loc[allDf.build_year==20052009,'build_year'] = 2005\nallDf.loc[allDf.build_year==4965,'build_year'] = np.nan\nallDf.loc[allDf.build_year>2021,'build_year'] = np.nan\nallDf.loc[allDf.num_room>15,'num_room'] = np.nan\nallDf.loc[allDf.num_room==0,'num_room'] = np.nan\nallDf.loc[allDf.floor==0,'floor'] = np.nan\nallDf.loc[allDf.max_floor==0,'max_floor'] = np.nan\nallDf.loc[allDf.floor>allDf.max_floor,'max_floor'] = np.nan\n\n# brings error down a lot by removing extreme price per sqm\nbad_index = allDf[allDf.price_doc/allDf.full_sq > 600000].index\nbad_index = bad_index.append(allDf[allDf.price_doc/allDf.full_sq < 10000].index)\nallDf.drop(bad_index,axis=0,inplace=True)\n\n####Feature Engineering####\nprint('Feature Engineering...')\ngc.collect()\n\nallDf['year'] = allDf.timestamp.dt.year  #may be no use because test data is out of range\nallDf['weekday'] = allDf.timestamp.dt.weekday\n\n# Assign weight\nallDf['w'] = 1\nallDf.loc[allDf.price_doc==1000000,'w'] *= 0.5\nallDf.loc[allDf.year==2015,'w'] *= 1.5\n\n#Floor\nallDf['floor_by_max_floor'] = allDf.floor / allDf.max_floor\n#allDf['floor_to_top'] = allDf.max_floor - allDf.floor\n\n#Room\nallDf['avg_room_size'] = (allDf.life_sq - allDf.kitch_sq) / allDf.num_room\nallDf['life_sq_prop'] = allDf.life_sq / allDf.full_sq\nallDf['kitch_sq_prop'] = allDf.kitch_sq / allDf.full_sq\n\n#Calculate age of building\nallDf['build_age'] = allDf.year - allDf.build_year\nallDf = allDf.drop(['build_year'], axis=1)\n\n#Population\nallDf['popu_den'] = allDf.raion_popul / allDf.area_m\nallDf['gender_rate'] = allDf.male_f / allDf.female_f\nallDf['working_rate'] = allDf.work_all / allDf.full_all\n\n#Education\nallDf.loc[allDf.preschool_quota==0,'preschool_quota'] = np.nan\nallDf['preschool_ratio'] =  allDf.children_preschool / allDf.preschool_quota\nallDf['school_ratio'] = allDf.children_school / allDf.school_quota\n\nallDf['square_full_sq'] = (allDf.full_sq - allDf.full_sq.mean()) ** 2\nallDf['square_build_age'] = (allDf.build_age - allDf.build_age.mean()) ** 2\nallDf['nan_count'] = allDf[['full_sq','build_age','life_sq','floor','max_floor','num_room']].isnull().sum(axis=1)\nallDf['full*maxfloor'] = allDf.max_floor * allDf.full_sq\nallDf['full*floor'] = allDf.floor * allDf.full_sq\n\nallDf['full/age'] = allDf.full_sq / (allDf.build_age + 0.5)\nallDf['age*state'] = allDf.build_age * allDf.state\n\n# new trial\nallDf['main_road_diff'] = allDf['big_road2_km'] - allDf['big_road1_km']\nallDf['rate_metro_km'] = allDf['metro_km_walk'] / allDf['ID_metro'].map(allDf.metro_km_walk.groupby(allDf.ID_metro).mean().to_dict())\nallDf['rate_road1_km'] = allDf['big_road1_km'] / allDf['ID_big_road1'].map(allDf.big_road1_km.groupby(allDf.ID_big_road1).mean().to_dict())\n# best on LB with weekday\n\nallDf['rate_road2_km'] = allDf['big_road2_km'] / allDf['ID_big_road2'].map(allDf.big_road2_km.groupby(allDf.ID_big_road2).mean().to_dict())\nallDf['rate_railroad_km'] = allDf['railroad_station_walk_km'] / allDf['ID_railroad_station_walk'].map(allDf.railroad_station_walk_km.groupby(allDf.ID_railroad_station_walk).mean().to_dict())\n\nallDf.drop(['year','timestamp'], axis=1, inplace = True)\n\n#Separate train and test again\ntrainDf = allDf[allDf.isTrain==1].drop(['isTrain'],axis=1)\ntestDf = allDf[allDf.isTrain==0].drop(['isTrain','price_doc', 'w'],axis=1)\n\noutputFile = 'train_featured.csv'\ntrainDf.to_csv(outputFile,index=False)\noutputFile = 'test_featured.csv'\ntestDf.to_csv(outputFile,index=False)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-19T07:22:14.638855Z","iopub.execute_input":"2023-11-19T07:22:14.639546Z","iopub.status.idle":"2023-11-19T07:22:29.132591Z","shell.execute_reply.started":"2023-11-19T07:22:14.639502Z","shell.execute_reply":"2023-11-19T07:22:29.130878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LGBregressor(object):\n    def __init__(self,params):\n        self.params = params\n\n    def fit(self, X, y, w):\n        y /= 10000000\n        split = int(X.shape[0] * 0.8)\n        indices = np.random.permutation(X.shape[0])\n        train_id, test_id = indices[:split], indices[split:]\n        x_train, y_train, w_train, x_valid, y_valid,  w_valid = X[train_id], y[train_id], w[train_id], X[test_id], y[test_id], w[test_id],\n        d_train = lgb.Dataset(x_train, y_train, weight=w_train)\n        d_valid = lgb.Dataset(x_valid, y_valid, weight=w_valid)\n        partial_bst = lgb.train(self.params, d_train, 10000, valid_sets=d_valid, early_stopping_rounds=50)\n        num_round = partial_bst.best_iteration\n        d_all = lgb.Dataset(X, label = y, weight=w)\n        self.bst = lgb.train(self.params, d_all, num_round)\n\n    def predict(self, X):\n        return self.bst.predict(X) * 10000000\n\nclass XGBregressor(object):\n    def __init__(self, params):\n        self.params = params\n\n    def fit(self, X, y, w=None):\n        if w is None:\n            w = np.ones(X.shape[0])\n        split = int(X.shape[0] * 0.8)\n        indices = np.random.permutation(X.shape[0])\n        train_id, test_id = indices[:split], indices[split:]\n        x_train, y_train, w_train, x_valid, y_valid,  w_valid = X[train_id], y[train_id], w[train_id], X[test_id], y[test_id], w[test_id],\n        d_train = xgb.DMatrix(x_train, label=y_train, weight=w_train)\n        d_valid = xgb.DMatrix(x_valid, label=y_valid, weight=w_valid)\n        watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n        partial_bst = xgb.train(self.params, d_train, 10000, early_stopping_rounds=50, evals = watchlist, verbose_eval=100)\n        num_round = partial_bst.best_iteration\n        d_all = xgb.DMatrix(X, label = y, weight=w)\n        self.bst = xgb.train(self.params, d_all, num_round)\n\n    def predict(self, X):\n        test = xgb.DMatrix(X)\n        return self.bst.predict(test)\n\nclass Ensemble(object):\n    def __init__(self, n_folds, stacker, base_models):\n        self.n_folds = n_folds\n        self.stacker = stacker\n        self.base_models = base_models\n\n    def fit_predict(self, trainDf, testDf):\n        X = trainDf.drop(['price_doc', 'w'], axis=1).values\n        y = trainDf['price_doc'].values\n        w = trainDf['w'].values\n        T = testDf.values\n\n        X_fillna = trainDf.drop(['price_doc', 'w'], axis=1).fillna(-999).values\n        T_fillna = testDf.fillna(-999).values\n\n        folds = list(KFold(n_splits=self.n_folds, shuffle=True).split(X))\n        S_train = np.zeros((X.shape[0], len(self.base_models)))\n        S_test = np.zeros((T.shape[0], len(self.base_models)))\n        for i, clf in enumerate(self.base_models):\n            print('Training base model ' + str(i+1) + '...')\n            S_test_i = np.zeros((T.shape[0], len(folds)))\n            for j, (train_idx, test_idx) in enumerate(folds):\n                print('Training round ' + str(j+1) + '...')\n                if clf not in [xgb1,lgb1]: # sklearn models cannot handle missing values.\n                    X = X_fillna\n                    T = T_fillna\n                X_train = X[train_idx]\n                y_train = y[train_idx]\n                w_train = w[train_idx]\n                X_holdout = X[test_idx]\n                clf.fit(X_train, y_train, w_train)\n                y_pred = clf.predict(X_holdout)\n                S_train[test_idx, i] = y_pred\n                S_test_i[:, j] = clf.predict(T)\n            S_test[:, i] = S_test_i.mean(1)\n        self.S_train, self.S_test, self.y = S_train, S_test, y\n        self.corr = pd.concat([pd.DataFrame(S_train),trainDf['price_doc']],axis=1).corr()\n        self.stacker.fit(S_train, y)\n        y_pred = self.stacker.predict(S_test)\n        return y_pred\n\ntrainDf = pd.read_csv('/kaggle/working/train_featured.csv')\ntestDf = pd.read_csv('/kaggle/working/test_featured.csv')\n\nparams1 = {'eta':0.05, 'max_depth':5, 'subsample':0.8, 'colsample_bytree':0.8, 'min_child_weight':1,\n              'gamma':0, 'silent':1, 'objective':'reg:linear', 'eval_metric':'rmse'}\nxgb1 = XGBregressor(params1)\n\nparams2 = {'booster':'gblinear', 'alpha':0,\n           'eta':0.1, 'max_depth':2, 'subsample':1, 'colsample_bytree':1, 'min_child_weight':1,\n            'gamma':0, 'silent':1, 'objective':'reg:linear', 'eval_metric':'rmse'}\nxgb2_meta = XGBregressor(params2)\n\nparams_lgb = {'objective':'regression','metric':'rmse',\n              'learning_rate':0.05,'max_depth':-1,'sub_feature':0.7,'sub_row':1,\n              'num_leaves':15,'min_data':30,'max_bin':20,\n              'bagging_fraction':0.9,'bagging_freq':40,'verbosity':0}\nlgb1 = LGBregressor(params_lgb)\n\nRF = RandomForestRegressor(n_estimators=500, max_features=0.2)\nETR = ExtraTreesRegressor(n_estimators=500, max_features=0.3, max_depth=None)\nAda = AdaBoostRegressor(DecisionTreeRegressor(max_depth=15),n_estimators=200)\nGBR = GradientBoostingRegressor(n_estimators=200,max_depth=5,max_features=0.5)\nLR =LinearRegression()\n    \nE = Ensemble(n_folds=5, stacker=xgb2_meta, base_models=[xgb1,lgb1,RF,ETR,Ada,GBR])\nprediction = E.fit_predict(trainDf, testDf)\noutput = pd.read_csv('test.csv')\noutput = output[['id']]\noutput['price_doc'] = prediction\noutput.to_csv(r'Ensemble\\Submission_Stack.csv',index=False)\n\ncorr = pd.concat([pd.DataFrame(S_train),trainDf['price_doc']],axis=1).corr()","metadata":{"execution":{"iopub.status.busy":"2023-11-19T07:36:18.518194Z","iopub.execute_input":"2023-11-19T07:36:18.518823Z"},"trusted":true},"execution_count":null,"outputs":[]}]}